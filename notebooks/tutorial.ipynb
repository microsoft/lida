{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIDA - Automatic Generation of Visualizations and Infographics using Large Language Models\n",
    "\n",
    "LIDA is a library for generating data visualizations and data-faithful infographics. LIDA is grammar agnostic (will work with any programming language and visualization libraries e.g. matplotlib, seaborn, altair, d3 etc) and works with multiple large language model providers (OpenAI, PaLM, Cohere, Huggingface). Details on the components of LIDA are described in the [paper here](https://arxiv.org/abs/2303.02927) and in this tutorial [notebook](notebooks/tutorial.ipynb). See the project page [here](https://microsoft.github.io/lida/) for updates!.\n",
    "\n",
    "\n",
    "\n",
    "## Getting Started | Installation\n",
    "\n",
    "```bash \n",
    "pip install lida\n",
    "```\n",
    "\n",
    "If you intend to use lida with local huggingface models, you will need to install the `transformers` library. \n",
    "\n",
    "```bash\n",
    "pip install lida[transformers]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The LIDA Python API\n",
    "\n",
    "Lida offers a manager class that exposes core functionality of the LIDA system. This tutorial will show you how to use the manager class to create visualizations based on a dataset.\n",
    "\n",
    "### Multiple LLM Backends\n",
    "LIDA supports multiple LLM backends such as `openai`, `cohere`, `palm`, `huggingface` etc. You can switch between backends by setting the `text_gen` parameter in the `Manager` class. By default, LIDA uses the `openai` backend. For a list of supported models and how to configure them, see the [llmx documentation](https://github.com/victordibia/llmx).\n",
    "\n",
    "```python\n",
    "\n",
    "from lida import llm\n",
    "\n",
    "text_gen = llm(\"openai\") # for openai\n",
    "text_gen = llm(provider=\"openai\", api_type=\"azure\", api_base=os.environ[\"AZURE_OPENAI_BASE\"], api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],    api_version=\"2023-07-01-preview\") # for azure openai\n",
    "text_gen = llm(\"cohere\") # for cohere\n",
    "text_gen = llm(\"palm\") # for palm\n",
    "text_gen = llm(provider=\"hf\", model=\"uukuguy/speechless-llama2-hermes-orca-platypus-13b\", device_map=\"auto\")\n",
    "\n",
    "lida = Manager(text_gen=text_gen)\n",
    "```\n",
    "\n",
    "Note that you can set your llm keys as follows\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_KEY=<your key>\n",
    "export COHERE_API_KEY=<your key>\n",
    "# for PaLM\n",
    "export PALM_SERVICE_ACCOUNT_KEY_FILE=<path to gcp service account key file>\n",
    "export PALM_PROJECT_ID=<your gcp project id>\n",
    "```\n",
    "#### Azure OpenAI\n",
    "```python\n",
    "from llmx.generators.text.openai_textgen import OpenAITextGenerator\n",
    "from llmx.generators.text.textgen import sanitize_provider\n",
    "\n",
    "provider = sanitize_provider(\"azureopenai\")\n",
    "models = {}\n",
    "\n",
    "text_gen = OpenAITextGenerator(\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_base=os.environ[\"AZURE_OPENAI_BASE\"],\n",
    "    provider=provider,\n",
    "    models=models\n",
    ")\n",
    "lida = Manager(text_gen=text_gen)\n",
    "```\n",
    "\n",
    "\n",
    "### Summarization Methods \n",
    "The summarizer module works takes an `summary_method` argument which determines if the base summary is enriched by an LLM. By default, the `summary_method` argument is set to `default` for a base summary (statistics etc). Set it to `llm` to enrich/annotate the base summary with an llm.\n",
    "\n",
    "### Caching \n",
    "Each manager method takes a [`textgen_config`](https://github.com/victordibia/llmx/blob/7c0fc093d1b8780ebebc7e080f5c63991514038b/llmx/datamodel.py#L22C10-L22C10) argument which is a dictionary that can be used to configure the text generation process (with parameters for model, temperature, max_tokens, topk etc). One of the keys in this dictionary is `use_cache`. If set to `True`, the manager will cache the generated text associated with that method. Use for speedup and to avoid hitting API limits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lida \n",
    "# !pip install lida[infographics] # for infographics support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lida import Manager, TextGenerationConfig , llm  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Data, Generate Goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lida = Manager(text_gen = llm(\"openai\", api_key=None)) # !! api key\n",
    "textgen_config = TextGenerationConfig(n=1, temperature=0.5, model=\"gpt-3.5-turbo-0301\", use_cache=True)\n",
    "\n",
    "summary = lida.summarize(\"https://raw.githubusercontent.com/uwdata/draco/master/data/cars.csv\", summary_method=\"default\", textgen_config=textgen_config)  \n",
    "goals = lida.goals(summary, n=2, textgen_config=textgen_config)\n",
    "\n",
    "for goal in goals:\n",
    "    display(goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# goals can also be based on a persona \n",
    "persona = \"a mechanic who wants to buy a car that is cheap but has good gas mileage\"\n",
    "personal_goals = lida.goals(summary, n=2, persona=persona, textgen_config=textgen_config)\n",
    "for goal in personal_goals:\n",
    "    display(goal)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "library = \"seaborn\"\n",
    "textgen_config = TextGenerationConfig(n=1, temperature=0.2, use_cache=True)\n",
    "charts = lida.visualize(summary=summary, goal=goals[i], textgen_config=textgen_config, library=library)  \n",
    "charts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate visualization via a \"user query\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What is the average price of cars by type?\"\n",
    "textgen_config = TextGenerationConfig(n=1, temperature=0.2, use_cache=True)\n",
    "charts = lida.visualize(summary=summary, goal=user_query, textgen_config=textgen_config)  \n",
    "charts[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VizOps\n",
    "\n",
    "Given that LIDA represents visualizations as code,\n",
    "the VISGENERATOR also implements submodules\n",
    "to perform operations on this representation. \n",
    "\n",
    "This includes \n",
    "- **Natural language based visualization refinement**: Provides a conversational api to iteratively\n",
    "4Execution in a sandbox environment is recommended.\n",
    "refine generated code (e.g., translate chart t hindi\n",
    ". . . zoom in by 50% etc) which can then be executed to generate new visualizations.\n",
    "- **Visualization explanations and accessibility**:\n",
    "Generates natural language explanations (valuable\n",
    "for debugging and sensemaking) as well as accessibility descriptions (valuable for supporting users\n",
    "with visual impairments).\n",
    "\n",
    "- **Visualization code self-evaluation and repair**:\n",
    "Applies an LLM to self-evaluate generated code on\n",
    "multiple dimensions (see section 4.1.2).\n",
    "\n",
    "- **Visualization recommendation**: Given some context (goals, or an existing visualization), recommend additional visualizations to the user (e.g., for\n",
    "comparison, or to provide additional perspectives).\n",
    "\n",
    "\n",
    "\n",
    "## Natural language based visualization refinement \n",
    "\n",
    "Given some code, modify it based on natural language instructions. This yields a new code snippet that can be executed to generate a new visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = charts[0].code\n",
    "textgen_config = TextGenerationConfig(n=1, temperature=0, use_cache=True)\n",
    "instructions = [\"make the chart height and width equal\", \"change the color of the chart to red\", \"translate the chart to spanish\"]\n",
    "edited_charts = lida.edit(code=code,  summary=summary, instructions=instructions, library=library, textgen_config=textgen_config)\n",
    "edited_charts[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization explanations and accessibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations = lida.explain(code=code, library=library, textgen_config=textgen_config) \n",
    "for row in explanations[0]:\n",
    "    print(row[\"section\"],\" ** \", row[\"explanation\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization code self-evaluation and repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = lida.evaluate(code=code,  goal=goals[i], textgen_config=textgen_config, library=library)[0] \n",
    "for eval in evaluations:\n",
    "    print(eval[\"dimension\"], \"Score\" ,eval[\"score\"], \"/ 10\")\n",
    "    print(\"\\t\", eval[\"rationale\"][:200])\n",
    "    print(\"\\t**********************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textgen_config = TextGenerationConfig(n=2, temperature=0.2, use_cache=True)\n",
    "recommended_charts =  lida.recommend(code=code, summary=summary, n=2,  textgen_config=textgen_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Recommended {len(recommended_charts)} charts\")\n",
    "for chart in recommended_charts:\n",
    "    display(chart) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infographics (Beta)\n",
    "\n",
    "- Explores using LIDA to generate infographics from an existing visualization \n",
    "- Uses the `peacasso` package, and loads open source stable diffusion models \n",
    "- You will need to run `pip install lida[infographics]` to install the required dependencies.\n",
    "- Currently work in progress (work being done to post process infographics with chart axis and title overlays from the original visualization, add presets for different infographic styles, and add more stable diffusion models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lida[infographics] \n",
    "# ensure you have a GPU runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infographics = lida.infographics(visualization = edited_charts[0].raster, n=1, style_prompt=\"pastel art, green pearly rain drops, highly detailed, no blur, white background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lida.utils import plot_raster\n",
    "plot_raster([edited_charts[0].raster, infographics[\"images\"][0]]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "189101fc34b85ec7417252a331b6b3ef556b71030ac1f6fe00bfbe1409305460"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
